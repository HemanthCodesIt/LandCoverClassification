{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satellite Imagery Land Use Classification - Exploration Notebook\n",
    "\n",
    "This notebook provides an interactive exploration of the EuroSAT dataset and demonstrates how to use deep learning for satellite imagery classification.\n",
    "\n",
    "## What you'll learn:\n",
    "1. How to load and visualize satellite imagery data\n",
    "2. Understanding the EuroSAT dataset structure\n",
    "3. Data preprocessing and augmentation techniques\n",
    "4. Model training and evaluation\n",
    "5. Visualizing results and predictions\n",
    "\n",
    "## Dataset Overview\n",
    "The EuroSAT dataset contains 27,000 labeled satellite images from Sentinel-2 covering 10 different land use classes:\n",
    "- Annual Crop\n",
    "- Forest\n",
    "- Herbaceous Vegetation\n",
    "- Highway\n",
    "- Industrial Buildings\n",
    "- Pasture\n",
    "- Permanent Crop\n",
    "- Residential Buildings\n",
    "- River\n",
    "- Sea Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "def visualize_predictions(model, data_loader, num_samples=8):\n",
    "    \"\"\"Visualize model predictions\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch\n",
    "    images, true_labels = next(iter(data_loader))\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        predicted_labels = outputs.argmax(dim=1)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        img = images[i]\n",
    "        true_label = true_labels[i].item()\n",
    "        pred_label = predicted_labels[i].item()\n",
    "        \n",
    "        # Denormalize image\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        img = img * std + mean\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        # Plot\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        axes[i].imshow(img_np)\n",
    "        \n",
    "        # Color code: green for correct, red for incorrect\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        axes[i].set_title(\n",
    "            f'True: {class_names[true_label]}\\nPred: {class_names[pred_label]}',\n",
    "            color=color, fontsize=9\n",
    "        )\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Model Predictions (Green=Correct, Red=Incorrect)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample predictions from the demo model:\")\n",
    "visualize_predictions(demo_trainer.model, quick_val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze what features the model has learned\n",
    "def visualize_conv_filters(model, layer_name='conv1'):\n",
    "    \"\"\"Visualize convolutional filters from the first layer\"\"\"\n",
    "    \n",
    "    # Get the first convolutional layer\n",
    "    conv_layer = getattr(model, layer_name)\n",
    "    filters = conv_layer.weight.data.clone()\n",
    "    \n",
    "    # Normalize filters for visualization\n",
    "    filters = filters - filters.min()\n",
    "    filters = filters / filters.max()\n",
    "    \n",
    "    # Plot filters\n",
    "    num_filters = min(16, filters.shape[0])  # Show first 16 filters\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(num_filters):\n",
    "        # Convert filter to RGB if it has 3 channels\n",
    "        if filters.shape[1] == 3:\n",
    "            filter_img = filters[i].permute(1, 2, 0).numpy()\n",
    "        else:\n",
    "            filter_img = filters[i, 0].numpy()\n",
    "        \n",
    "        axes[i].imshow(filter_img, cmap='viridis' if len(filter_img.shape) == 2 else None)\n",
    "        axes[i].set_title(f'Filter {i+1}', fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Learned Filters from {layer_name}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize learned filters\n",
    "print(\"Learned convolutional filters:\")\n",
    "visualize_conv_filters(demo_trainer.model, 'conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-world Applications and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize per-class performance\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Demo Model')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(class_names, class_accuracies)\n",
    "plt.title('Per-Class Accuracy')\n",
    "plt.xlabel('Land Use Classes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, class_accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "            f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What we've accomplished:\n",
    "1. âœ… Loaded and explored the EuroSAT satellite imagery dataset\n",
    "2. âœ… Visualized different land use classes and their characteristics\n",
    "3. âœ… Demonstrated data augmentation techniques\n",
    "4. âœ… Compared different model architectures\n",
    "5. âœ… Trained a demo model and evaluated its performance\n",
    "6. âœ… Visualized model predictions and learned features\n",
    "\n",
    "### Applications to Indian Context:\n",
    "The techniques learned here can be directly applied to:\n",
    "- **Agricultural Monitoring**: Crop classification using Indian satellite data\n",
    "- **Urban Planning**: Building and infrastructure detection in Indian cities\n",
    "- **Environmental Studies**: Forest cover analysis and deforestation monitoring\n",
    "- **Disaster Management**: Flood mapping and damage assessment\n",
    "\n",
    "### Next Steps:\n",
    "1. **Full Training**: Run the complete training script with more epochs\n",
    "2. **Experiment**: Try different model architectures and hyperparameters\n",
    "3. **Indian Data**: Apply these techniques to Indian satellite imagery\n",
    "4. **Advanced Techniques**: Explore transfer learning and ensemble methods\n",
    "5. **Deployment**: Create a web application for real-time classification\n",
    "\n",
    "### Learning Resources:\n",
    "- Sentinel-2 satellite data: https://scihub.copernicus.eu/\n",
    "- Indian satellite data: https://bhuvan.nrsc.gov.in/\n",
    "- Deep learning for remote sensing: Research papers and tutorials\n",
    "- PyTorch documentation: https://pytorch.org/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"ðŸŽ‰ Exploration Complete!\")\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(f\"- Dataset size: {len(train_loader) * BATCH_SIZE + len(val_loader) * BATCH_SIZE + len(test_loader) * BATCH_SIZE} images\")\n",
    "print(f\"- Number of classes: {len(class_names)}\")\n",
    "print(f\"- Demo model accuracy: {accuracy:.4f}\")\n",
    "print(f\"- Best performing class: {class_names[class_accuracies.argmax()]} ({class_accuracies.max():.3f})\")\n",
    "print(f\"- Most challenging class: {class_names[class_accuracies.argmin()]} ({class_accuracies.min():.3f})\")\n",
    "\n",
    "print(\"\\nReady to train a full model? Run:\")\n",
    "print(\"python scripts/run_training.py --epochs 20 --model resnet18\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our modules\n",
    "from data_loader import create_data_loaders, EuroSATDataset, visualize_samples\n",
    "from model import get_model, model_summary\n",
    "from train import create_trainer\n",
    "from evaluate import ModelEvaluator\n",
    "from utils import set_seed, get_device\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {get_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = '../data'\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 2\n",
    "INPUT_SIZE = 64\n",
    "\n",
    "# Create data loaders\n",
    "print(\"Creating data loaders...\")\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    input_size=INPUT_SIZE,\n",
    "    download=False  # Assume data is already downloaded\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Get class names\n",
    "class_names = EuroSATDataset.CLASSES\n",
    "print(f\"\\nClasses ({len(class_names)}): {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset distribution\n",
    "def analyze_dataset_distribution(data_loader, title=\"Dataset\"):\n",
    "    \"\"\"Analyze and plot class distribution in dataset\"\"\"\n",
    "    all_labels = []\n",
    "    \n",
    "    for _, labels in data_loader:\n",
    "        all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Count occurrences\n",
    "    label_counts = Counter(all_labels)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    classes = [class_names[i] for i in sorted(label_counts.keys())]\n",
    "    counts = [label_counts[i] for i in sorted(label_counts.keys())]\n",
    "    \n",
    "    bars = plt.bar(classes, counts)\n",
    "    plt.title(f'{title} - Class Distribution')\n",
    "    plt.xlabel('Land Use Classes')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                str(count), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return label_counts\n",
    "\n",
    "# Analyze training set distribution\n",
    "train_distribution = analyze_dataset_distribution(train_loader, \"Training Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "print(\"Sample images from the training set:\")\n",
    "visualize_samples(train_loader, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more detailed visualization showing one sample from each class\n",
    "def visualize_one_per_class(data_loader, class_names):\n",
    "    \"\"\"Visualize one sample from each class\"\"\"\n",
    "    # Dictionary to store one sample per class\n",
    "    class_samples = {}\n",
    "    \n",
    "    # Iterate through data loader to find one sample per class\n",
    "    for images, labels in data_loader:\n",
    "        for img, label in zip(images, labels):\n",
    "            label_idx = label.item()\n",
    "            if label_idx not in class_samples:\n",
    "                class_samples[label_idx] = img\n",
    "                \n",
    "            # Break if we have all classes\n",
    "            if len(class_samples) == len(class_names):\n",
    "                break\n",
    "        \n",
    "        if len(class_samples) == len(class_names):\n",
    "            break\n",
    "    \n",
    "    # Plot samples\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (label_idx, img) in enumerate(sorted(class_samples.items())):\n",
    "        # Denormalize image\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        img = img * std + mean\n",
    "        img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        # Convert to numpy and plot\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        axes[i].imshow(img_np)\n",
    "        axes[i].set_title(class_names[label_idx], fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Images from Each Land Use Class', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_one_per_class(train_loader, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate data augmentation effects\n",
    "from torchvision import transforms\n",
    "\n",
    "def show_augmentation_effects(original_image):\n",
    "    \"\"\"Show effects of different augmentation techniques\"\"\"\n",
    "    \n",
    "    # Define different augmentation transforms\n",
    "    augmentations = {\n",
    "        'Original': transforms.Compose([]),\n",
    "        'Horizontal Flip': transforms.RandomHorizontalFlip(p=1.0),\n",
    "        'Vertical Flip': transforms.RandomVerticalFlip(p=1.0),\n",
    "        'Rotation': transforms.RandomRotation(degrees=30),\n",
    "        'Color Jitter': transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "        'All Combined': transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (name, transform) in enumerate(augmentations.items()):\n",
    "        # Apply transform\n",
    "        if name == 'Original':\n",
    "            img = original_image\n",
    "        else:\n",
    "            # Convert to PIL for transforms\n",
    "            img_pil = transforms.ToPILImage()(original_image)\n",
    "            img_pil = transform(img_pil)\n",
    "            img = transforms.ToTensor()(img_pil)\n",
    "        \n",
    "        # Plot\n",
    "        img_np = img.permute(1, 2, 0).numpy()\n",
    "        axes[i].imshow(img_np)\n",
    "        axes[i].set_title(name)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Data Augmentation Effects', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get a sample image\n",
    "for images, labels in train_loader:\n",
    "    sample_image = images[0]\n",
    "    sample_label = labels[0]\n",
    "    \n",
    "    # Denormalize for visualization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    sample_image = sample_image * std + mean\n",
    "    sample_image = torch.clamp(sample_image, 0, 1)\n",
    "    \n",
    "    print(f\"Demonstrating augmentation on: {class_names[sample_label]}\")\n",
    "    show_augmentation_effects(sample_image)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different model architectures\n",
    "models_to_compare = ['simple_cnn', 'resnet18', 'attention_cnn']\n",
    "\n",
    "print(\"Model Architecture Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    model = get_model(model_name, num_classes=10, pretrained=False)\n",
    "    model_summary(model, input_size=(3, INPUT_SIZE, INPUT_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Training Demo (Mini Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training demo with a simple model for 3 epochs\n",
    "print(\"Quick Training Demo (3 epochs with simple CNN)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a smaller dataset for quick demo\n",
    "quick_train_loader, quick_val_loader, _ = create_data_loaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=32,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_split=0.1,  # Use only 10% of data for quick demo\n",
    "    input_size=INPUT_SIZE,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "# Create trainer with simple CNN\n",
    "demo_trainer = create_trainer(\n",
    "    model_name='simple_cnn',\n",
    "    train_loader=quick_train_loader,\n",
    "    val_loader=quick_val_loader,\n",
    "    learning_rate=0.01,  # Higher learning rate for quick demo\n",
    "    save_dir='./demo_models'\n",
    ")\n",
    "\n",
    "# Train for 3 epochs\n",
    "demo_history = demo_trainer.train(num_epochs=3)\n",
    "\n",
    "print(\"\\nDemo training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the demo training history\n",
    "from train import plot_training_history\n",
    "\n",
    "print(\"Demo Training Results:\")\n",
    "plot_training_history(demo_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the demo model\n",
    "evaluator = ModelEvaluator(demo_trainer.model)\n",
    "\n",
    "# Make predictions on validation set\n",
    "predictions, labels = evaluator.predict(quick_val_loader)\n",
    "\n",
    "# Calculate basic metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score
